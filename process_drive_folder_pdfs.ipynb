{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# PDF Processing Pipeline — Self-contained Notebook (GPU-ready)\n",
    "\n",
    "This notebook is **fully self-contained**. All pipeline code is embedded inline —\n",
    "no external `pipeline/` package is needed. It installs every dependency, mounts\n",
    "Google Drive, finds all PDFs in the folder you specify, and runs the full flow:\n",
    "\n",
    "**PDF → Markdown → NLP → Chunking → Vector DB → Manifest**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def _has(mod: str) -> bool:\n",
    "    return importlib.util.find_spec(mod) is not None\n",
    "\n",
    "\n",
    "PACKAGES = [\n",
    "    (\"docling\", \"docling\"),\n",
    "    (\"tqdm\", \"tqdm\"),\n",
    "    (\"spacy\", \"spacy\"),\n",
    "    (\"rake-nltk\", \"rake_nltk\"),\n",
    "    (\"vaderSentiment\", \"vaderSentiment\"),\n",
    "    (\"scikit-learn\", \"sklearn\"),\n",
    "    (\"nltk\", \"nltk\"),\n",
    "    (\"qdrant-client\", \"qdrant_client\"),\n",
    "    (\"fastembed\", \"fastembed\"),\n",
    "    (\"chromadb\", \"chromadb\"),\n",
    "    (\"sentence-transformers\", \"sentence_transformers\"),\n",
    "]\n",
    "\n",
    "missing = [pip for pip, mod in PACKAGES if not _has(mod)]\n",
    "if missing:\n",
    "    print(f\"Installing: {missing}\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *missing])\n",
    "else:\n",
    "    print(\"All packages already installed.\")\n",
    "\n",
    "if not _has(\"en_core_web_sm\"):\n",
    "    print(\"Downloading spaCy model: en_core_web_sm\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "\n",
    "print(\"Dependency setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  USER CONFIGURATION — edit these before running             ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "FOLDER_NAME = \"YOUR_DRIVE_FOLDER_NAME\"\n",
    "BACKEND = \"qdrant\"  # \"qdrant\" or \"chroma\"\n",
    "RUN_NLP = True\n",
    "\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
    "OUTPUT_BASE = Path(\"/content/output\")\n",
    "QDRANT_PATH = Path(\"/content/qdrant_data\")\n",
    "\n",
    "assert FOLDER_NAME and FOLDER_NAME != \"YOUR_DRIVE_FOLDER_NAME\", \"Set FOLDER_NAME first\"\n",
    "assert BACKEND in {\"qdrant\", \"chroma\"}, \"BACKEND must be 'qdrant' or 'chroma'\"\n",
    "assert DRIVE_ROOT.exists(), f\"Drive root not found: {DRIVE_ROOT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "log = logging.getLogger(\"pipeline\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Constants\n",
    "# ---------------------------------------------------------------------------\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "EMBEDDING_DIM = 384\n",
    "MAX_TOKENS = 512\n",
    "COLLECTION_NAME = \"docling_documents\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Data model\n",
    "# ---------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class DocRecord:\n",
    "    \"\"\"Tracks conversion results and metadata for a single PDF.\"\"\"\n",
    "\n",
    "    filename: str\n",
    "    filepath: str\n",
    "    drive_file_id: str = \"\"\n",
    "    markdown: str = \"\"\n",
    "    num_pages: int = 0\n",
    "    num_tables: int = 0\n",
    "    num_figures: int = 0\n",
    "    title: str = \"\"\n",
    "    conversion_time_s: float = 0.0\n",
    "    status: str = \"pending\"\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "print(\"Models and constants loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab() -> bool:\n",
    "    \"\"\"Detect if running inside Google Colab.\"\"\"\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "\n",
    "def ensure_output_dirs(output_dir: Path) -> tuple[Path, Path]:\n",
    "    \"\"\"Create and return (markdown_dir, db_dir) under *output_dir*.\"\"\"\n",
    "    md_dir = output_dir / \"markdown\"\n",
    "    db_dir = output_dir / \"vector_db\"\n",
    "    md_dir.mkdir(parents=True, exist_ok=True)\n",
    "    db_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return md_dir, db_dir\n",
    "\n",
    "\n",
    "def save_manifest(\n",
    "    output_dir: Path,\n",
    "    records: list[DocRecord],\n",
    "    nlp_results: dict[str, dict[str, Any]],\n",
    ") -> Path:\n",
    "    \"\"\"Write document_manifest.json and return its path.\"\"\"\n",
    "    manifest: list[dict[str, Any]] = []\n",
    "    for record in records:\n",
    "        entry: dict[str, Any] = {\n",
    "            \"filename\": record.filename,\n",
    "            \"filepath\": record.filepath,\n",
    "            \"title\": record.title,\n",
    "            \"status\": record.status,\n",
    "            \"num_pages\": record.num_pages,\n",
    "            \"num_tables\": record.num_tables,\n",
    "            \"num_figures\": record.num_figures,\n",
    "            \"conversion_time_s\": record.conversion_time_s,\n",
    "            \"error\": record.error,\n",
    "        }\n",
    "        if record.filename in nlp_results:\n",
    "            entry[\"nlp_analysis\"] = nlp_results[record.filename]\n",
    "        manifest.append(entry)\n",
    "\n",
    "    manifest_path = output_dir / \"document_manifest.json\"\n",
    "    with open(manifest_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(manifest, fh, indent=2, ensure_ascii=False, default=str)\n",
    "    return manifest_path\n",
    "\n",
    "\n",
    "def discover_pdfs(folder: Path) -> list[Path]:\n",
    "    \"\"\"Recursively find all PDF files under *folder*, sorted by name.\"\"\"\n",
    "    if not folder.exists():\n",
    "        return []\n",
    "    return sorted(folder.rglob(\"*.pdf\"))\n",
    "\n",
    "\n",
    "print(\"Utility functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vlm_converter() -> tuple[Any, str]:\n",
    "    \"\"\"Build a Docling DocumentConverter with VLM pipeline.\"\"\"\n",
    "    import time as _time\n",
    "\n",
    "    t0 = _time.time()\n",
    "    log.info(\"create_vlm_converter: importing docling modules ...\")\n",
    "\n",
    "    from docling.datamodel.base_models import InputFormat\n",
    "    from docling.datamodel.pipeline_options import VlmConvertOptions, VlmPipelineOptions\n",
    "    from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "    from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "\n",
    "    log.info(f\"create_vlm_converter: imports done in {_time.time() - t0:.2f}s\")\n",
    "\n",
    "    t1 = _time.time()\n",
    "    try:\n",
    "        log.info(\"create_vlm_converter: trying smoldocling preset ...\")\n",
    "        vlm_options = VlmConvertOptions.from_preset(\"smoldocling\")\n",
    "        vlm_name = \"SmolDocling\"\n",
    "        log.info(\n",
    "            f\"create_vlm_converter: smoldocling loaded in {_time.time() - t1:.2f}s\"\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        log.warning(\n",
    "            f\"create_vlm_converter: smoldocling failed ({exc}), trying granite_docling ...\"\n",
    "        )\n",
    "        vlm_options = VlmConvertOptions.from_preset(\"granite_docling\")\n",
    "        vlm_name = \"GraniteDocling\"\n",
    "        log.info(\n",
    "            f\"create_vlm_converter: granite_docling loaded in {_time.time() - t1:.2f}s\"\n",
    "        )\n",
    "\n",
    "    t2 = _time.time()\n",
    "    pipeline_options = VlmPipelineOptions(vlm_options=vlm_options)\n",
    "    log.info(\n",
    "        f\"create_vlm_converter: VlmPipelineOptions done in {_time.time() - t2:.2f}s\"\n",
    "    )\n",
    "\n",
    "    t3 = _time.time()\n",
    "    converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                pipeline_cls=VlmPipeline,\n",
    "                pipeline_options=pipeline_options,\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    log.info(\n",
    "        f\"create_vlm_converter: DocumentConverter created in {_time.time() - t3:.2f}s\"\n",
    "    )\n",
    "    log.info(\n",
    "        f\"Docling VLM converter initialized ({vlm_name}) — total {_time.time() - t0:.2f}s.\"\n",
    "    )\n",
    "    return converter, vlm_name\n",
    "\n",
    "\n",
    "def convert_single_pdf(converter: Any, pdf_path: Path) -> tuple[DocRecord, Any]:\n",
    "    \"\"\"Convert one PDF and return (DocRecord, DoclingDocument | None).\"\"\"\n",
    "    log.info(\n",
    "        f\"convert_single_pdf: START — {pdf_path.name} \"\n",
    "        f\"({pdf_path.stat().st_size if pdf_path.exists() else 'MISSING'} bytes)\"\n",
    "    )\n",
    "    record = DocRecord(filename=pdf_path.name, filepath=str(pdf_path))\n",
    "    doc = None\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        result = converter.convert(source=str(pdf_path))\n",
    "        doc = result.document\n",
    "        record.markdown = doc.export_to_markdown()\n",
    "        record.num_pages = len(result.pages) if hasattr(result, \"pages\") else 0\n",
    "        record.num_tables = sum(1 for _ in doc.tables) if hasattr(doc, \"tables\") else 0\n",
    "        record.num_figures = (\n",
    "            sum(1 for _ in doc.pictures) if hasattr(doc, \"pictures\") else 0\n",
    "        )\n",
    "        record.title = doc.name if hasattr(doc, \"name\") and doc.name else pdf_path.stem\n",
    "        record.status = \"success\"\n",
    "        log.info(\n",
    "            f\"convert_single_pdf: SUCCESS — pages={record.num_pages}, \"\n",
    "            f\"tables={record.num_tables}, figures={record.num_figures}\"\n",
    "        )\n",
    "    except Exception:\n",
    "        record.status = \"error\"\n",
    "        record.error = traceback.format_exc()\n",
    "        log.error(f\"convert_single_pdf: ERROR — {record.error}\")\n",
    "    finally:\n",
    "        record.conversion_time_s = round(time.time() - t0, 2)\n",
    "        log.info(\n",
    "            f\"convert_single_pdf: DONE — {pdf_path.name} in {record.conversion_time_s}s\"\n",
    "        )\n",
    "    return record, doc\n",
    "\n",
    "\n",
    "def convert_pdfs(\n",
    "    converter: Any,\n",
    "    pdf_files: list[Path],\n",
    "    md_output_dir: Path,\n",
    ") -> tuple[list[DocRecord], dict[str, Any]]:\n",
    "    \"\"\"Batch-convert PDFs. Returns (records, {filename: DoclingDocument}).\"\"\"\n",
    "    records: list[DocRecord] = []\n",
    "    docling_docs: dict[str, Any] = {}\n",
    "\n",
    "    for pdf_path in pdf_files:\n",
    "        record, doc = convert_single_pdf(converter, pdf_path)\n",
    "        records.append(record)\n",
    "        if record.status == \"success\" and doc is not None:\n",
    "            md_file = md_output_dir / f\"{pdf_path.stem}.md\"\n",
    "            md_file.write_text(record.markdown, encoding=\"utf-8\")\n",
    "            docling_docs[record.filename] = doc\n",
    "\n",
    "    success = [r for r in records if r.status == \"success\"]\n",
    "    failed = [r for r in records if r.status == \"error\"]\n",
    "    log.info(f\"Conversion: {len(success)} succeeded, {len(failed)} failed\")\n",
    "    return records, docling_docs\n",
    "\n",
    "\n",
    "print(\"Conversion functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_nlp_model = None\n",
    "_rake_model = None\n",
    "_vader_model = None\n",
    "\n",
    "\n",
    "def _get_spacy():\n",
    "    global _nlp_model\n",
    "    if _nlp_model is None:\n",
    "        import spacy\n",
    "\n",
    "        _nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "    return _nlp_model\n",
    "\n",
    "\n",
    "def _get_rake():\n",
    "    global _rake_model\n",
    "    if _rake_model is None:\n",
    "        from rake_nltk import Rake\n",
    "\n",
    "        _rake_model = Rake()\n",
    "    return _rake_model\n",
    "\n",
    "\n",
    "def _get_vader():\n",
    "    global _vader_model\n",
    "    if _vader_model is None:\n",
    "        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "        _vader_model = SentimentIntensityAnalyzer()\n",
    "    return _vader_model\n",
    "\n",
    "\n",
    "def init_nlp() -> tuple:\n",
    "    \"\"\"Load all NLP models once. Returns (spacy_nlp, rake, vader).\"\"\"\n",
    "    import nltk\n",
    "\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    nltk.download(\"punkt_tab\", quiet=True)\n",
    "    return _get_spacy(), _get_rake(), _get_vader()\n",
    "\n",
    "\n",
    "def extract_keywords_rake(text: str, top_n: int = 15) -> list[str]:\n",
    "    \"\"\"Extract keywords using RAKE algorithm.\"\"\"\n",
    "    rake = _get_rake()\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    return rake.get_ranked_phrases()[:top_n]\n",
    "\n",
    "\n",
    "def extract_entities(text: str, max_chars: int = 100_000) -> dict[str, list[str]]:\n",
    "    \"\"\"Extract named entities grouped by label using spaCy.\"\"\"\n",
    "    nlp = _get_spacy()\n",
    "    doc = nlp(text[:max_chars])\n",
    "    entities: dict[str, set[str]] = {}\n",
    "    for ent in doc.ents:\n",
    "        entities.setdefault(ent.label_, set()).add(ent.text.strip())\n",
    "    return {k: sorted(v)[:20] for k, v in entities.items()}\n",
    "\n",
    "\n",
    "def extractive_summary(text: str, num_sentences: int = 5) -> str:\n",
    "    \"\"\"Fast extractive summary — first N non-trivial sentences.\"\"\"\n",
    "    nlp = _get_spacy()\n",
    "    doc = nlp(text[:50_000])\n",
    "    sentences = [s.text.strip() for s in doc.sents if len(s.text.strip()) > 40]\n",
    "    return \" \".join(sentences[:num_sentences])\n",
    "\n",
    "\n",
    "def analyze_sentiment(text: str, analyzer: Any = None) -> dict[str, Any]:\n",
    "    \"\"\"Sentiment analysis via VADER (default) or a HuggingFace pipeline.\"\"\"\n",
    "    if analyzer is not None:\n",
    "        result = analyzer(text[:2000])\n",
    "        return {\"label\": result[0][\"label\"], \"score\": round(result[0][\"score\"], 4)}\n",
    "\n",
    "    vader = _get_vader()\n",
    "    scores = vader.polarity_scores(text[:5000])\n",
    "    compound = scores[\"compound\"]\n",
    "    if compound >= 0.05:\n",
    "        label = \"POSITIVE\"\n",
    "    elif compound <= -0.05:\n",
    "        label = \"NEGATIVE\"\n",
    "    else:\n",
    "        label = \"NEUTRAL\"\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"score\": round(abs(compound), 4),\n",
    "        \"compound\": round(compound, 4),\n",
    "        \"scores\": scores,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_tfidf_topics(texts: list[str], top_n: int = 10) -> list[list[str]]:\n",
    "    \"\"\"Extract top TF-IDF terms per document for topic signals.\"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    if not texts:\n",
    "        return []\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        stop_words=\"english\",\n",
    "        max_df=0.85,\n",
    "        min_df=1,\n",
    "    )\n",
    "    matrix = vectorizer.fit_transform(texts)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    topics: list[list[str]] = []\n",
    "    for i in range(matrix.shape[0]):\n",
    "        row = matrix[i].toarray().flatten()\n",
    "        top_idx = row.argsort()[-top_n:][::-1]\n",
    "        topics.append([features[j] for j in top_idx if row[j] > 0])\n",
    "    return topics\n",
    "\n",
    "\n",
    "def classify_document_type(text: str) -> str:\n",
    "    \"\"\"Heuristic document type classification based on content signals.\"\"\"\n",
    "    text_lower = text[:10_000].lower()\n",
    "    patterns = {\n",
    "        \"lease\": [\"lease\", \"landlord\", \"tenant\", \"rent\", \"premises\"],\n",
    "        \"contract\": [\n",
    "            \"agreement\",\n",
    "            \"parties\",\n",
    "            \"hereby\",\n",
    "            \"whereas\",\n",
    "            \"terms and conditions\",\n",
    "        ],\n",
    "        \"invoice\": [\"invoice\", \"amount due\", \"bill to\", \"payment terms\", \"total due\"],\n",
    "        \"legal_notice\": [\"notice\", \"hereby notified\", \"pursuant to\", \"demand\"],\n",
    "        \"tax_document\": [\n",
    "            \"tax\",\n",
    "            \"assessment\",\n",
    "            \"property tax\",\n",
    "            \"assessed value\",\n",
    "            \"taxable\",\n",
    "        ],\n",
    "        \"title_report\": [\"title\", \"escrow\", \"deed\", \"recording\", \"conveyance\"],\n",
    "        \"amendment\": [\"amendment\", \"first amendment\", \"second amendment\", \"modify\"],\n",
    "        \"letter\": [\"dear\", \"sincerely\", \"regards\", \"attention\"],\n",
    "        \"report\": [\"report\", \"findings\", \"analysis\", \"recommendation\"],\n",
    "        \"insurance\": [\"insurance\", \"policy\", \"premium\", \"coverage\", \"claim\"],\n",
    "    }\n",
    "    scores = {\n",
    "        doc_type: sum(1 for t in terms if t in text_lower)\n",
    "        for doc_type, terms in patterns.items()\n",
    "    }\n",
    "    best = max(scores, key=scores.get)\n",
    "    return best if scores[best] >= 2 else \"general\"\n",
    "\n",
    "\n",
    "def run_nlp_analysis(\n",
    "    records: list[DocRecord],\n",
    "    sentiment_analyzer: Any = None,\n",
    ") -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"Run full NLP pipeline on successful records.\"\"\"\n",
    "    successful = [r for r in records if r.status == \"success\" and r.markdown]\n",
    "    all_texts = [r.markdown for r in successful]\n",
    "    all_tfidf = extract_tfidf_topics(all_texts)\n",
    "\n",
    "    results: dict[str, dict[str, Any]] = {}\n",
    "    for i, record in enumerate(successful):\n",
    "        text = record.markdown\n",
    "        entities = extract_entities(text)\n",
    "        results[record.filename] = {\n",
    "            \"keywords_rake\": extract_keywords_rake(text),\n",
    "            \"named_entities\": entities,\n",
    "            \"summary\": extractive_summary(text),\n",
    "            \"sentiment\": analyze_sentiment(text, analyzer=sentiment_analyzer),\n",
    "            \"tfidf_topics\": all_tfidf[i] if i < len(all_tfidf) else [],\n",
    "            \"document_type\": classify_document_type(text),\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"char_count\": len(text),\n",
    "            \"people\": entities.get(\"PERSON\", []),\n",
    "            \"organizations\": entities.get(\"ORG\", []),\n",
    "            \"dates\": entities.get(\"DATE\", []),\n",
    "            \"amounts\": entities.get(\"MONEY\", []),\n",
    "        }\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"NLP functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunker(\n",
    "    embedding_model: str = EMBEDDING_MODEL,\n",
    "    max_tokens: int = MAX_TOKENS,\n",
    ") -> Any:\n",
    "    \"\"\"Create a Docling HybridChunker aligned to the embedding model tokenizer.\"\"\"\n",
    "    from docling.chunking import HybridChunker\n",
    "\n",
    "    return HybridChunker(\n",
    "        tokenizer=embedding_model,\n",
    "        max_tokens=max_tokens,\n",
    "        merge_peers=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "    chunker: Any,\n",
    "    records: list[DocRecord],\n",
    "    docling_docs: dict[str, Any],\n",
    "    nlp_results: dict[str, dict[str, Any]],\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Chunk all successful documents and return flat list of chunk dicts.\"\"\"\n",
    "    all_chunks: list[dict[str, Any]] = []\n",
    "\n",
    "    for record in records:\n",
    "        if record.status != \"success\":\n",
    "            continue\n",
    "        doc = docling_docs.get(record.filename)\n",
    "        if doc is None:\n",
    "            continue\n",
    "\n",
    "        analysis = nlp_results.get(record.filename, {})\n",
    "\n",
    "        try:\n",
    "            for chunk_idx, chunk in enumerate(chunker.chunk(dl_doc=doc)):\n",
    "                contextualized_text = chunker.contextualize(chunk)\n",
    "                chunk_data = {\n",
    "                    \"id\": f\"{record.filename}::chunk_{chunk_idx:04d}\",\n",
    "                    \"text\": contextualized_text,\n",
    "                    \"raw_text\": chunk.text,\n",
    "                    \"source_file\": record.filename,\n",
    "                    \"source_path\": record.filepath,\n",
    "                    \"doc_title\": record.title,\n",
    "                    \"num_pages\": record.num_pages,\n",
    "                    \"num_tables\": record.num_tables,\n",
    "                    \"num_figures\": record.num_figures,\n",
    "                    \"chunk_index\": chunk_idx,\n",
    "                    \"headings\": (\n",
    "                        \" > \".join(chunk.meta.headings) if chunk.meta.headings else \"\"\n",
    "                    ),\n",
    "                    \"keywords\": \", \".join(analysis.get(\"keywords_rake\", [])[:10]),\n",
    "                    \"tfidf_topics\": \", \".join(analysis.get(\"tfidf_topics\", [])[:10]),\n",
    "                    \"sentiment_label\": analysis.get(\"sentiment\", {}).get(\"label\", \"\"),\n",
    "                    \"sentiment_score\": analysis.get(\"sentiment\", {}).get(\"score\", 0.0),\n",
    "                    \"doc_summary\": analysis.get(\"summary\", \"\")[:500],\n",
    "                    \"word_count\": analysis.get(\"word_count\", 0),\n",
    "                }\n",
    "                all_chunks.append(chunk_data)\n",
    "        except Exception as exc:\n",
    "            log.warning(f\"Chunking failed for {record.filename}: {exc}\")\n",
    "            continue\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "print(\"Chunking functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_collection(\n",
    "    db_dir: Path,\n",
    "    collection_name: str = COLLECTION_NAME,\n",
    "    embedding_model: str = EMBEDDING_MODEL,\n",
    ") -> tuple[Any, Any]:\n",
    "    \"\"\"Create persistent ChromaDB client + collection.\"\"\"\n",
    "    import chromadb\n",
    "    from chromadb.utils import embedding_functions\n",
    "\n",
    "    client = chromadb.PersistentClient(path=str(db_dir))\n",
    "    embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=embedding_model,\n",
    "    )\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_fn,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "    )\n",
    "    return client, collection\n",
    "\n",
    "\n",
    "def insert_chunks(\n",
    "    collection: Any,\n",
    "    chunks: list[dict[str, Any]],\n",
    "    batch_size: int = 50,\n",
    ") -> int:\n",
    "    \"\"\"Upsert chunks into a ChromaDB collection. Returns total count.\"\"\"\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i : i + batch_size]\n",
    "        ids = [c[\"id\"] for c in batch]\n",
    "        documents = [c[\"text\"] for c in batch]\n",
    "        metadatas = [\n",
    "            {\n",
    "                \"source_file\": c[\"source_file\"],\n",
    "                \"source_path\": c[\"source_path\"],\n",
    "                \"doc_title\": c[\"doc_title\"],\n",
    "                \"num_pages\": c[\"num_pages\"],\n",
    "                \"num_tables\": c[\"num_tables\"],\n",
    "                \"num_figures\": c[\"num_figures\"],\n",
    "                \"chunk_index\": c[\"chunk_index\"],\n",
    "                \"headings\": c[\"headings\"],\n",
    "                \"keywords\": c[\"keywords\"],\n",
    "                \"tfidf_topics\": c[\"tfidf_topics\"],\n",
    "                \"sentiment_label\": c[\"sentiment_label\"],\n",
    "                \"sentiment_score\": c[\"sentiment_score\"],\n",
    "                \"doc_summary\": c[\"doc_summary\"],\n",
    "                \"word_count\": c[\"word_count\"],\n",
    "            }\n",
    "            for c in batch\n",
    "        ]\n",
    "        collection.upsert(ids=ids, documents=documents, metadatas=metadatas)\n",
    "    return collection.count()\n",
    "\n",
    "\n",
    "def build_qdrant_collection(\n",
    "    qdrant_path: Path,\n",
    "    chunker: Any,\n",
    "    records: list,\n",
    "    docling_docs: dict[str, Any],\n",
    "    nlp_results: dict[str, dict],\n",
    "    collection_name: str = COLLECTION_NAME,\n",
    "    embedding_model_id: str = EMBEDDING_MODEL,\n",
    "    embedding_dim: int = EMBEDDING_DIM,\n",
    ") -> int:\n",
    "    \"\"\"Chunk documents and insert into Qdrant with FastEmbed embeddings.\"\"\"\n",
    "    from fastembed import TextEmbedding\n",
    "    from qdrant_client import QdrantClient\n",
    "    from qdrant_client.models import Distance, PointStruct, VectorParams\n",
    "\n",
    "    client = QdrantClient(path=str(qdrant_path))\n",
    "\n",
    "    if client.collection_exists(collection_name):\n",
    "        client.delete_collection(collection_name)\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "    successful = [r for r in records if r.status == \"success\"]\n",
    "    all_documents: list[str] = []\n",
    "    all_metadata: list[dict] = []\n",
    "\n",
    "    for record in successful:\n",
    "        doc = docling_docs.get(record.filename)\n",
    "        if doc is None:\n",
    "            continue\n",
    "        analysis = nlp_results.get(record.filename, {})\n",
    "\n",
    "        for chunk_idx, chunk in enumerate(chunker.chunk(dl_doc=doc)):\n",
    "            ctx_text = chunker.contextualize(chunk)\n",
    "            metadata = {\n",
    "                \"text\": ctx_text,\n",
    "                \"source_file\": record.filename,\n",
    "                \"source_path\": record.filepath,\n",
    "                \"doc_title\": record.title,\n",
    "                \"chunk_index\": chunk_idx,\n",
    "                \"headings\": (\n",
    "                    \" > \".join(chunk.meta.headings) if chunk.meta.headings else \"\"\n",
    "                ),\n",
    "                \"keywords\": \", \".join(analysis.get(\"keywords_rake\", [])[:10]),\n",
    "                \"tfidf_topics\": \", \".join(analysis.get(\"tfidf_topics\", [])[:10]),\n",
    "                \"document_type\": analysis.get(\"document_type\", \"\"),\n",
    "                \"sentiment_label\": analysis.get(\"sentiment\", {}).get(\"label\", \"\"),\n",
    "                \"sentiment_compound\": analysis.get(\"sentiment\", {}).get(\n",
    "                    \"compound\", 0.0\n",
    "                ),\n",
    "                \"doc_summary\": analysis.get(\"summary\", \"\")[:500],\n",
    "                \"people\": \", \".join(analysis.get(\"people\", [])[:10]),\n",
    "                \"organizations\": \", \".join(analysis.get(\"organizations\", [])[:10]),\n",
    "                \"dates\": \", \".join(analysis.get(\"dates\", [])[:10]),\n",
    "                \"amounts\": \", \".join(analysis.get(\"amounts\", [])[:10]),\n",
    "                \"word_count\": analysis.get(\"word_count\", 0),\n",
    "            }\n",
    "            all_documents.append(ctx_text)\n",
    "            all_metadata.append(metadata)\n",
    "\n",
    "    if not all_documents:\n",
    "        log.warning(\"No chunks to insert\")\n",
    "        client.close()\n",
    "        return 0\n",
    "\n",
    "    log.info(f\"Embedding {len(all_documents)} chunks with {embedding_model_id}...\")\n",
    "    embedding_model = TextEmbedding(embedding_model_id)\n",
    "    embeddings = list(embedding_model.embed(all_documents))\n",
    "\n",
    "    batch_size = 64\n",
    "    for i in range(0, len(all_documents), batch_size):\n",
    "        batch_end = min(i + batch_size, len(all_documents))\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=i + j,\n",
    "                vector=embeddings[i + j].tolist(),\n",
    "                payload=all_metadata[i + j],\n",
    "            )\n",
    "            for j in range(batch_end - i)\n",
    "        ]\n",
    "        client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "    count = client.count(collection_name=collection_name).count\n",
    "    client.close()\n",
    "    return count\n",
    "\n",
    "\n",
    "print(\"Vector store functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def resolve_folder_by_name(drive_root: Path, folder_name: str) -> Path:\n",
    "    \"\"\"Find a folder by name under drive_root (direct child first, then recursive).\"\"\"\n",
    "    if folder_name.startswith(\"/\"):\n",
    "        candidate = Path(folder_name)\n",
    "        if candidate.exists() and candidate.is_dir():\n",
    "            return candidate\n",
    "\n",
    "    direct = drive_root / folder_name\n",
    "    if direct.exists() and direct.is_dir():\n",
    "        return direct\n",
    "\n",
    "    matches = sorted(\n",
    "        [p for p in drive_root.rglob(\"*\") if p.is_dir() and p.name == folder_name]\n",
    "    )\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No folder named '{folder_name}' found under {drive_root}\"\n",
    "        )\n",
    "\n",
    "    if len(matches) > 1:\n",
    "        print(\"Multiple folders found with same name; using first match:\")\n",
    "        for idx, path in enumerate(matches[:10], start=1):\n",
    "            print(f\"  {idx}. {path}\")\n",
    "\n",
    "    return matches[0]\n",
    "\n",
    "\n",
    "target_folder = resolve_folder_by_name(DRIVE_ROOT, FOLDER_NAME)\n",
    "run_slug = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = OUTPUT_BASE / f\"{target_folder.name}_{run_slug}\"\n",
    "md_dir, db_dir = ensure_output_dirs(output_dir)\n",
    "nlp_dir = output_dir / \"nlp\"\n",
    "nlp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_files = discover_pdfs(target_folder)\n",
    "if not pdf_files:\n",
    "    raise RuntimeError(\"No PDFs found in selected folder\")\n",
    "\n",
    "print(f\"Resolved folder: {target_folder}\")\n",
    "print(f\"PDF count:       {len(pdf_files)}\")\n",
    "print(f\"Output dir:      {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter, vlm_name = create_vlm_converter()\n",
    "records, docling_docs = convert_pdfs(\n",
    "    converter=converter,\n",
    "    pdf_files=pdf_files,\n",
    "    md_output_dir=md_dir,\n",
    ")\n",
    "\n",
    "success = [r for r in records if r.status == \"success\"]\n",
    "failed = [r for r in records if r.status == \"error\"]\n",
    "print(f\"VLM preset:  {vlm_name}\")\n",
    "print(f\"Conversion:  {len(success)} success, {len(failed)} failed\")\n",
    "\n",
    "nlp_results: dict[str, dict] = {}\n",
    "if RUN_NLP:\n",
    "    _ = init_nlp()\n",
    "    nlp_results = run_nlp_analysis(records)\n",
    "\n",
    "    for filename, analysis in nlp_results.items():\n",
    "        nlp_file = nlp_dir / f\"{Path(filename).stem}_nlp.json\"\n",
    "        with open(nlp_file, \"w\", encoding=\"utf-8\") as handle:\n",
    "            json.dump(analysis, handle, indent=2, default=str)\n",
    "\n",
    "print(f\"NLP analysis: {len(nlp_results)} document(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = create_chunker()\n",
    "\n",
    "if BACKEND == \"qdrant\":\n",
    "    vector_count = build_qdrant_collection(\n",
    "        qdrant_path=QDRANT_PATH,\n",
    "        chunker=chunker,\n",
    "        records=records,\n",
    "        docling_docs=docling_docs,\n",
    "        nlp_results=nlp_results,\n",
    "    )\n",
    "else:\n",
    "    chunks = chunk_documents(chunker, records, docling_docs, nlp_results)\n",
    "    _, collection = create_chroma_collection(db_dir)\n",
    "    vector_count = insert_chunks(collection, chunks)\n",
    "\n",
    "manifest_path = save_manifest(output_dir, records, nlp_results)\n",
    "\n",
    "total_time = sum(r.conversion_time_s for r in records)\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(f\"  PDFs processed:  {len(records)}\")\n",
    "print(f\"  Succeeded:       {len(success)}\")\n",
    "print(f\"  Failed:          {len(failed)}\")\n",
    "print(f\"  Vectors stored:  {vector_count}\")\n",
    "print(f\"  Markdown dir:    {md_dir}\")\n",
    "print(f\"  Manifest:        {manifest_path}\")\n",
    "print(f\"  Total conv time: {total_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if failed:\n",
    "    print(\"\\nFailed files:\")\n",
    "    for record in failed:\n",
    "        print(f\"- {record.filename}\")\n",
    "        print((record.error or \"unknown error\")[:1000])\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"All files processed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}