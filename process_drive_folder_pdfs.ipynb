{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# PDF Processing Pipeline — Notebook Interface\n",
    "\n",
    "This notebook is a **thin wrapper** around the `pipeline/` package. All pipeline\n",
    "code lives in the package modules — this notebook only handles configuration,\n",
    "Colab environment setup, and orchestration.\n",
    "\n",
    "It installs every dependency, optionally mounts Google Drive, finds all PDFs in\n",
    "the folder you specify, and runs the full flow:\n",
    "\n",
    "**PDF → Markdown → NLP → Chunking → Vector DB → Manifest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def _has(mod: str) -> bool:\n",
    "    return importlib.util.find_spec(mod) is not None\n",
    "\n",
    "\n",
    "# Full dependency list matching pyproject.toml\n",
    "PACKAGES = [\n",
    "    (\"docling\", \"docling\"),\n",
    "    (\"docling-core\", \"docling_core\"),\n",
    "    (\"easyocr\", \"easyocr\"),\n",
    "    (\"google-api-python-client\", \"googleapiclient\"),\n",
    "    (\"google-auth-httplib2\", \"google_auth_httplib2\"),\n",
    "    (\"google-auth-oauthlib\", \"google_auth_oauthlib\"),\n",
    "    (\"nltk\", \"nltk\"),\n",
    "    (\"opencv-python-headless==4.13.0.92\", \"cv2\"),\n",
    "    (\"qdrant-client[fastembed]\", \"qdrant_client\"),\n",
    "    (\"rake-nltk\", \"rake_nltk\"),\n",
    "    (\"scikit-learn\", \"sklearn\"),\n",
    "    (\"sentence-transformers\", \"sentence_transformers\"),\n",
    "    (\"spacy\", \"spacy\"),\n",
    "    (\"torch\", \"torch\"),\n",
    "    (\"tqdm\", \"tqdm\"),\n",
    "    (\"transformers\", \"transformers\"),\n",
    "    (\"vaderSentiment\", \"vaderSentiment\"),\n",
    "    (\"chromadb\", \"chromadb\"),\n",
    "]\n",
    "\n",
    "missing = [pip for pip, mod in PACKAGES if not _has(mod)]\n",
    "if missing:\n",
    "    print(f\"Installing: {missing}\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *missing])\n",
    "else:\n",
    "    print(\"All packages already installed.\")\n",
    "\n",
    "if not _has(\"en_core_web_sm\"):\n",
    "    print(\"Downloading spaCy model: en_core_web_sm\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "\n",
    "print(\"Dependency setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  USER CONFIGURATION — edit these before running             ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "FOLDER_NAME = \"output\"\n",
    "BACKEND = \"qdrant\"  # \"qdrant\" or \"chroma\"\n",
    "RUN_NLP = True\n",
    "DISABLE_OCR = False\n",
    "FORCE_REPROCESS = False\n",
    "REBUILD_INDEX = False\n",
    "\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
    "OUTPUT_BASE = Path(\"/content/output\")\n",
    "QDRANT_PATH = Path(\"/content/qdrant_data\")\n",
    "\n",
    "assert FOLDER_NAME, \"Set FOLDER_NAME first\"\n",
    "assert BACKEND in {\"qdrant\", \"chroma\"}, \"BACKEND must be 'qdrant' or 'chroma'\"\n",
    "assert DRIVE_ROOT.exists(), f\"Drive root not found: {DRIVE_ROOT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport logging\nimport os\nimport time\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Any\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlog = logging.getLogger(\"pipeline\")\n\n# Ensure the project root (containing pipeline/) is on sys.path.\n# In Colab: clone the repo first, then set PROJECT_ROOT to the clone path.\n# Locally: defaults to the notebook's working directory.\nPROJECT_ROOT = os.environ.get(\"PROJECT_ROOT\", os.getcwd())\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\n# Import everything from the pipeline package\nfrom pipeline import (\n    COLLECTION_NAME,\n    EMBEDDING_DIM,\n    EMBEDDING_MODEL,\n    MAX_TOKENS,\n    DocRecord,\n    analyze_sentiment,\n    build_qdrant_collection,\n    chunk_documents,\n    classify_document_type,\n    convert_single_pdf,\n    create_chroma_collection,\n    create_chunker,\n    create_ocr_converter,\n    discover_pdfs,\n    ensure_output_dirs,\n    extract_entities,\n    extract_keywords_rake,\n    extract_tfidf_topics,\n    extractive_summary,\n    init_nlp,\n    insert_chunks,\n    run_nlp_analysis,\n    save_manifest,\n)\nfrom pipeline.utils import (\n    file_fingerprint,\n    is_unchanged_file,\n    load_pipeline_state,\n    save_pipeline_state,\n)\n\nprint(f\"Pipeline loaded from: {PROJECT_ROOT}\")\nprint(\"Pipeline imports loaded.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_folder_by_name(drive_root: Path, folder_name: str) -> Path:\n",
    "    \"\"\"Find a folder by name under drive_root (direct child first, then recursive).\"\"\"\n",
    "    if folder_name.startswith(\"/\"):\n",
    "        candidate = Path(folder_name)\n",
    "        if candidate.exists() and candidate.is_dir():\n",
    "            return candidate\n",
    "\n",
    "    direct = drive_root / folder_name\n",
    "    if direct.exists() and direct.is_dir():\n",
    "        return direct\n",
    "\n",
    "    matches = sorted(\n",
    "        [p for p in drive_root.rglob(\"*\") if p.is_dir() and p.name == folder_name]\n",
    "    )\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No folder named '{folder_name}' found under {drive_root}\"\n",
    "        )\n",
    "\n",
    "    if len(matches) > 1:\n",
    "        print(\"Multiple folders found with same name; using first match:\")\n",
    "        for idx, path in enumerate(matches[:10], start=1):\n",
    "            print(f\"  {idx}. {path}\")\n",
    "\n",
    "    return matches[0]\n",
    "\n",
    "\n",
    "# --- Step 1: Resolve folder, create output dirs, discover PDFs ---\n",
    "target_folder = resolve_folder_by_name(DRIVE_ROOT, FOLDER_NAME)\n",
    "output_dir = OUTPUT_BASE / target_folder.name\n",
    "md_dir, db_dir = ensure_output_dirs(output_dir)\n",
    "nlp_dir = output_dir / \"nlp\"\n",
    "nlp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_files = discover_pdfs(target_folder)\n",
    "if not pdf_files:\n",
    "    raise RuntimeError(\"No PDFs found in selected folder\")\n",
    "\n",
    "# --- Resume check: skip unchanged PDFs ---\n",
    "state = load_pipeline_state(output_dir)\n",
    "state_files: dict[str, dict[str, Any]] = state.setdefault(\"files\", {})\n",
    "pdf_files_to_process: list[Path] = []\n",
    "skipped_unchanged = 0\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    state_key = str(pdf_path.resolve())\n",
    "    previous = state_files.get(state_key)\n",
    "    md_file = md_dir / f\"{pdf_path.stem}.md\"\n",
    "    nlp_file = nlp_dir / f\"{pdf_path.stem}_nlp.json\"\n",
    "    has_output_artifacts = md_file.exists() and (not RUN_NLP or nlp_file.exists())\n",
    "    if (\n",
    "        not FORCE_REPROCESS\n",
    "        and has_output_artifacts\n",
    "        and is_unchanged_file(pdf_path, previous)\n",
    "    ):\n",
    "        skipped_unchanged += 1\n",
    "        continue\n",
    "    pdf_files_to_process.append(pdf_path)\n",
    "\n",
    "print(f\"Resolved folder: {target_folder}\")\n",
    "print(f\"PDF count:       {len(pdf_files)}\")\n",
    "print(f\"Skipped cached:  {skipped_unchanged}\")\n",
    "print(f\"To process:      {len(pdf_files_to_process)}\")\n",
    "print(f\"Output dir:      {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# --- Step 2: Convert PDFs to Markdown ---\n",
    "docling_docs: dict[str, object] = {}\n",
    "records: list[DocRecord] = []\n",
    "\n",
    "if pdf_files_to_process:\n",
    "    log.info(\"Initializing Docling OCR converter...\")\n",
    "    converter, ocr_engine = create_ocr_converter(enable_ocr=not DISABLE_OCR)\n",
    "    log.info(f\"Converter OCR profile: {ocr_engine}\")\n",
    "\n",
    "    for pdf_path in tqdm(pdf_files_to_process, desc=\"Converting PDFs\"):\n",
    "        record, doc = convert_single_pdf(converter, pdf_path)\n",
    "        records.append(record)\n",
    "        if doc is not None:\n",
    "            docling_docs[record.filename] = doc\n",
    "            md_file = md_dir / f\"{pdf_path.stem}.md\"\n",
    "            md_file.write_text(record.markdown, encoding=\"utf-8\")\n",
    "else:\n",
    "    log.info(\"No new/changed PDFs detected; conversion stage skipped.\")\n",
    "\n",
    "success = [r for r in records if r.status == \"success\"]\n",
    "failed = [r for r in records if r.status == \"error\"]\n",
    "print(f\"OCR engine:  {ocr_engine if pdf_files_to_process else 'n/a'}\")\n",
    "print(f\"Conversion:  {len(success)} success, {len(failed)} failed\")\n",
    "\n",
    "# --- Step 3: NLP Analysis ---\n",
    "nlp_results: dict[str, dict] = {}\n",
    "if not RUN_NLP:\n",
    "    log.info(\"NLP stage disabled via RUN_NLP=False\")\n",
    "elif success:\n",
    "    log.info(\"Running NLP analysis...\")\n",
    "    init_nlp()\n",
    "\n",
    "    successful = [r for r in records if r.status == \"success\" and r.markdown]\n",
    "    all_texts = [r.markdown for r in successful]\n",
    "\n",
    "    # Batch TF-IDF (needs all texts at once)\n",
    "    log.info(\"Computing TF-IDF topics...\")\n",
    "    all_tfidf = extract_tfidf_topics(all_texts)\n",
    "\n",
    "    # Per-record NLP with progress bar\n",
    "    for i, record in enumerate(tqdm(successful, desc=\"NLP analysis\")):\n",
    "        text = record.markdown\n",
    "        entities = extract_entities(text)\n",
    "        analysis = {\n",
    "            \"keywords_rake\": extract_keywords_rake(text),\n",
    "            \"named_entities\": entities,\n",
    "            \"summary\": extractive_summary(text),\n",
    "            \"sentiment\": analyze_sentiment(text),\n",
    "            \"tfidf_topics\": all_tfidf[i] if i < len(all_tfidf) else [],\n",
    "            \"document_type\": classify_document_type(text),\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"char_count\": len(text),\n",
    "            \"people\": entities.get(\"PERSON\", []),\n",
    "            \"organizations\": entities.get(\"ORG\", []),\n",
    "            \"dates\": entities.get(\"DATE\", []),\n",
    "            \"amounts\": entities.get(\"MONEY\", []),\n",
    "        }\n",
    "        nlp_results[record.filename] = analysis\n",
    "\n",
    "        nlp_file = nlp_dir / f\"{Path(record.filename).stem}_nlp.json\"\n",
    "        with open(nlp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(analysis, f, indent=2, default=str)\n",
    "else:\n",
    "    log.info(\"No successful new/changed docs; NLP stage skipped.\")\n",
    "\n",
    "print(f\"NLP analysis: {len(nlp_results)} document(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Chunk and store in vector DB ---\n",
    "log.info(\"Chunking and building vector database...\")\n",
    "step4_t0 = time.perf_counter()\n",
    "chunker = create_chunker()\n",
    "\n",
    "if BACKEND == \"qdrant\":\n",
    "    log.info(\"Building Qdrant collection (chunking + embedding + upsert)...\")\n",
    "    vector_count = build_qdrant_collection(\n",
    "        qdrant_path=QDRANT_PATH,\n",
    "        chunker=chunker,\n",
    "        records=records,\n",
    "        docling_docs=docling_docs,\n",
    "        nlp_results=nlp_results,\n",
    "        recreate_collection=REBUILD_INDEX,\n",
    "    )\n",
    "    log.info(f\"Qdrant: {vector_count} vectors stored\")\n",
    "else:\n",
    "    log.info(\"Chunking documents...\")\n",
    "    chunks = chunk_documents(chunker, records, docling_docs, nlp_results)\n",
    "    log.info(f\"Created {len(chunks)} chunks, inserting into ChromaDB...\")\n",
    "    _, collection = create_chroma_collection(db_dir)\n",
    "    vector_count = insert_chunks(collection, chunks)\n",
    "    log.info(f\"ChromaDB: {vector_count} vectors stored\")\n",
    "\n",
    "step4_elapsed = time.perf_counter() - step4_t0\n",
    "log.info(f\"Vector stage completed in {step4_elapsed:.1f}s\")\n",
    "\n",
    "# --- Step 5: Save manifest ---\n",
    "manifest_path = save_manifest(output_dir, records, nlp_results)\n",
    "\n",
    "# --- Step 6: Save pipeline state ---\n",
    "current_keys = {str(path.resolve()) for path in pdf_files}\n",
    "for key in list(state_files):\n",
    "    if key not in current_keys:\n",
    "        state_files.pop(key, None)\n",
    "\n",
    "processed_at = datetime.now(timezone.utc).isoformat()\n",
    "for record in records:\n",
    "    state_key = str(Path(record.filepath).resolve())\n",
    "    entry: dict[str, Any] = {\n",
    "        \"filename\": record.filename,\n",
    "        \"status\": record.status,\n",
    "        \"processed_at\": processed_at,\n",
    "    }\n",
    "    file_path = Path(record.filepath)\n",
    "    if file_path.exists():\n",
    "        try:\n",
    "            entry.update(file_fingerprint(file_path))\n",
    "        except OSError:\n",
    "            pass\n",
    "    if record.status == \"error\" and record.error:\n",
    "        entry[\"error\"] = record.error[:500]\n",
    "    state_files[state_key] = entry\n",
    "\n",
    "state_path = save_pipeline_state(output_dir, state)\n",
    "\n",
    "# --- Summary ---\n",
    "total_time = sum(r.conversion_time_s for r in records)\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(f\"  PDFs discovered: {len(pdf_files)}\")\n",
    "print(f\"  Processed now:   {len(records)}\")\n",
    "print(f\"  Skipped cached:  {skipped_unchanged}\")\n",
    "print(f\"  Succeeded:       {len(success)}\")\n",
    "print(f\"  Failed:          {len(failed)}\")\n",
    "print(f\"  Vectors stored:  {vector_count}\")\n",
    "print(f\"  Markdown dir:    {md_dir}\")\n",
    "print(f\"  Manifest:        {manifest_path}\")\n",
    "print(f\"  State:           {state_path}\")\n",
    "print(f\"  Vector time:     {step4_elapsed:.1f}s\")\n",
    "print(f\"  Total conv time: {total_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if failed:\n",
    "    print(\"\\nFailed files:\")\n",
    "    for record in failed:\n",
    "        print(f\"- {record.filename}\")\n",
    "        print((record.error or \"unknown error\")[:1000])\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"All files processed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}